{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from hydra_utils import read_logs, get_missing_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locating the Data\n",
    "First things first: to get you oriented, let's check where our results are located. Use this script in combination with your command to see where the results should be if your naming matches the example config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['results/1_4/0', 'results/1_4/1', 'results/1_4/2', 'results/1_4/3', 'results/1_4/4', 'results/1_4/5', 'results/1_4/6', 'results/1_4/7', 'results/1_4/8', 'results/1_4/9', 'results/3_4/0', 'results/3_4/1', 'results/3_4/2', 'results/3_4/3', 'results/3_4/4', 'results/3_4/5', 'results/3_4/6', 'results/3_4/7', 'results/3_4/8', 'results/3_4/9', 'results/7_4/0', 'results/7_4/1', 'results/7_4/2', 'results/7_4/3', 'results/7_4/4', 'results/7_4/5', 'results/7_4/6', 'results/7_4/7', 'results/7_4/8', 'results/7_4/9']\n"
     ]
    }
   ],
   "source": [
    "command_str = \"python cli.py 'seed=range(0,10)' method=1,4,7\"\n",
    "seeds = range(0,10)\n",
    "methods = [1,3,7]\n",
    "benchmarks = [4]\n",
    "base_path = \"results\"\n",
    "experiment_directories = []\n",
    "\n",
    "for b in benchmarks:\n",
    "    for m in methods:\n",
    "        for s in seeds:\n",
    "            experiment_directories.append(os.path.join(base_path, f\"{m}_{b}\", f\"{s}\"))\n",
    "\n",
    "print(experiment_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if Everything Ran Successfully\n",
    "Now that we know the location of our data, we can check if it's complete. For this purpose, you need to define a function that takes a directory for a single run and returns a boolean signal if this run is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify which file to look into, here we write a 'done.txt.'\n",
    "# For other files we return None\n",
    "# Other options are things like checking if a checkpoint exists or if performance logging has reached a certain point\n",
    "def job_done(path_str):\n",
    "    if path_str.endswith(\"done.txt\"):\n",
    "        with open(os.path.join(path_str, \"done.txt\"), \"r\") as f:\n",
    "            content = f.read()\n",
    "            if \"yes\" in content:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/1_4/0\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/0\n",
      "[]\n",
      "None\n",
      "results/1_4/1\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/1\n",
      "[]\n",
      "None\n",
      "results/1_4/2\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/2\n",
      "[]\n",
      "None\n",
      "results/1_4/3\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/3\n",
      "[]\n",
      "None\n",
      "results/1_4/4\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/4\n",
      "[]\n",
      "None\n",
      "results/1_4/5\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/5\n",
      "[]\n",
      "None\n",
      "results/1_4/6\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/6\n",
      "[]\n",
      "None\n",
      "results/1_4/7\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/7\n",
      "[]\n",
      "None\n",
      "results/1_4/8\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/8\n",
      "[]\n",
      "None\n",
      "results/1_4/9\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/1_4/9\n",
      "[]\n",
      "None\n",
      "results/3_4/0\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/0\n",
      "[]\n",
      "None\n",
      "results/3_4/1\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/1\n",
      "[]\n",
      "None\n",
      "results/3_4/2\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/2\n",
      "[]\n",
      "None\n",
      "results/3_4/3\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/3\n",
      "[]\n",
      "None\n",
      "results/3_4/4\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/4\n",
      "[]\n",
      "None\n",
      "results/3_4/5\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/5\n",
      "[]\n",
      "None\n",
      "results/3_4/6\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/6\n",
      "[]\n",
      "None\n",
      "results/3_4/7\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/7\n",
      "[]\n",
      "None\n",
      "results/3_4/8\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/8\n",
      "[]\n",
      "None\n",
      "results/3_4/9\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/3_4/9\n",
      "[]\n",
      "None\n",
      "results/7_4/0\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/0\n",
      "[]\n",
      "None\n",
      "results/7_4/1\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/1\n",
      "[]\n",
      "None\n",
      "results/7_4/2\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/2\n",
      "[]\n",
      "None\n",
      "results/7_4/3\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/3\n",
      "[]\n",
      "None\n",
      "results/7_4/4\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/4\n",
      "[]\n",
      "None\n",
      "results/7_4/5\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/5\n",
      "[]\n",
      "None\n",
      "results/7_4/6\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/6\n",
      "[]\n",
      "None\n",
      "results/7_4/7\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/7\n",
      "[]\n",
      "None\n",
      "results/7_4/8\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/8\n",
      "[]\n",
      "None\n",
      "results/7_4/9\n",
      "/Users/theeimer/Documents/git/automl_repo_template/arlbench/arlbench/results/7_4/9\n",
      "[]\n",
      "None\n",
      "Done: 0/30\n",
      "Missing jobs:\n",
      "results/1_4/0\n",
      "results/1_4/1\n",
      "results/1_4/2\n",
      "results/1_4/3\n",
      "results/1_4/4\n",
      "results/1_4/5\n",
      "results/1_4/6\n",
      "results/1_4/7\n",
      "results/1_4/8\n",
      "results/1_4/9\n",
      "results/3_4/0\n",
      "results/3_4/1\n",
      "results/3_4/2\n",
      "results/3_4/3\n",
      "results/3_4/4\n",
      "results/3_4/5\n",
      "results/3_4/6\n",
      "results/3_4/7\n",
      "results/3_4/8\n",
      "results/3_4/9\n",
      "results/7_4/0\n",
      "results/7_4/1\n",
      "results/7_4/2\n",
      "results/7_4/3\n",
      "results/7_4/4\n",
      "results/7_4/5\n",
      "results/7_4/6\n",
      "results/7_4/7\n",
      "results/7_4/8\n",
      "results/7_4/9\n"
     ]
    }
   ],
   "source": [
    "missing = get_missing_jobs(\"results\", job_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Runscripts to Rerun Missing Runs\n",
    "Since it's possible some runs die before finishing, we need to rerun them at times. Here we can generate scripts to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for b in benchmarks:\n",
    "    for m in methods:\n",
    "        for s in seeds:\n",
    "            filepath = f\"missing_method_{m}_benchmark_{b}_seed_{s}.sh\"\n",
    "            command = f\"python cli.py seed={s} method={m} benchmark={b}\"\n",
    "            first = True\n",
    "            with open(filepath, \"a+\") as f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    slurm_string = f\"\"\"#!/bin/bash \\n#SBATCH --error={m}_{b}.err \\n#SBATCH --job-name=missing \\n#SBATCH --mem=10GB \\n#SBATCH --output={m}_{b}.out \\n#SBATCH --partition=ai,tnt \\n#SBATCH --time=1440 \\nconda activate my_env\"\"\"\n",
    "                    f.write(slurm_string)\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(command)\n",
    "                f.write(\"\\n\")\n",
    "            all_files.append(filepath)\n",
    "\n",
    "with open(\"submit_all_missing.sh\", \"a+\") as f:\n",
    "    for file in all_files:\n",
    "        f.write(f\"sbatch {file}\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# now we could 'sbatch submit_all_missing.sh' in the terminal to run the missing jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "There are multiple ways to then work with your data. Here we'll provide you with a way to load it into dataframes since we assume this is how you'll want to work with experiment data. If this doesn't work for you, it should be easy to configure, come talk to someone! \n",
    "There are three steps to data loading:\n",
    "1. Write functions loading all result files you're interested in. In this case, we want to load the performance and emissions\n",
    "2. Write any post-processing you want done with the loading. Here, we want to delete some fields to save disk space.\n",
    "3. Tell us where to find your results and watch them load :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define how and what to load\n",
    "# We need to specify which files this function applies to and return None for the rest\n",
    "# Then we can load it into a pandas dataframe however we want\n",
    "def read_performance(path):\n",
    "    if os.path.exists(path) and path.endswith(\"performance.csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def read_emissions(path):\n",
    "    if os.path.exists(path) and path.endswith(\"emissions.csv\"):\n",
    "        with open(path, \"r\") as f:\n",
    "            content = json.load(f)\n",
    "        return content\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we don't care about all of our emissions data, so we delete some of the columns\n",
    "def drop_columns(df):\n",
    "    df = df.drop(columns=[\"gpu_power\", \"country_name\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get our data! It will also be saved in a csv file so we won't need to reload it\n",
    "loaded_data = read_logs(\"results\", [read_performance, read_emissions], [drop_columns], \"run_data\")\n",
    "loaded_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_deepcave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
